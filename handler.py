#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
âœ‚ï¸ AnimeCut Serverless v9.0 PRO (MoviePy v2 + Numpy Fix + Full IA)
"""

import runpod
import os
import sys
import logging
import tempfile
import requests
import gc
import json
import time
import uuid
import math
import subprocess
from pathlib import Path
from typing import List, Dict, Optional, Tuple
import random

# Garante path de plugins se usar volume
sys.path.append("/runpod-volume/site-packages")

try:
    import cv2
    import numpy as np
    import torch
    from transformers import pipeline
    from ultralytics import YOLO
    CV2_AVAILABLE = True
except ImportError:
    CV2_AVAILABLE = False
    print("âš ï¸ Bibliotecas Turbo (YOLO/CV2) nÃ£o encontradas!")

# ==================== CONFIGURAÃ‡ÃƒO ====================
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger("AnimeCutPro")

TEMP_DIR = Path("/tmp/animecut")
OUTPUT_DIR = Path("/runpod-volume/output")
TEMP_DIR.mkdir(parents=True, exist_ok=True)
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

VOLUME_PATH = Path("/runpod-volume")
MODELS_PATH = VOLUME_PATH / "models"
QWEN_MODEL_PATH = MODELS_PATH / "Qwen2.5-7B-Instruct"
FONT_PATH = VOLUME_PATH / "fonts" / "impact.ttf"
FONT_PATH.parent.mkdir(parents=True, exist_ok=True)

# ==================== IMPORTS MOVIEPY V2 ====================
try:
    # MoviePy v2 Imports
    from moviepy import *
    from moviepy.video.fx import MirrorX, GammaCorr, ColorX, MultiplySpeed
    # fallback para compatibilidade se nomes mudarem na dev2
    try:
        from moviepy.editor import VideoFileClip, ImageClip, CompositeVideoClip, ColorClip, TextClip, AudioFileClip
        from moviepy.video.fx.all import speedx, mirror_x, gamma_corr, colorx
        MOVIEPY_V1_COMPAT = True
    except:
        MOVIEPY_V1_COMPAT = False
        
    MOVIEPY_AVAILABLE = True
except ImportError as e:
    MOVIEPY_AVAILABLE = False
    logger.error(f"âŒ MoviePy nÃ£o disponÃ­vel: {e}")

try:
    from PIL import Image, ImageDraw, ImageFont, ImageColor
    PIL_AVAILABLE = True
except ImportError:
    PIL_AVAILABLE = False

try:
    import torch
    from transformers import AutoModelForCausalLM, AutoTokenizer
    GPU_AVAILABLE = torch.cuda.is_available()
    DEVICE = "cuda" if GPU_AVAILABLE else "cpu"
    AI_AVAILABLE = True
except ImportError:
    AI_AVAILABLE = False
    GPU_AVAILABLE = False

# B2 Setup
s3_client = None
B2_AVAILABLE = False
try:
    import boto3
    from botocore.client import Config
    
    B2_KEY_ID = os.environ.get("B2_KEY_ID")
    B2_APP_KEY = os.environ.get("B2_APPLICATION_KEY")
    B2_ENDPOINT = os.environ.get("B2_ENDPOINT", "")
    B2_BUCKET = os.environ.get("B2_BUCKET_NAME", "KortexAI")
    
    # ValidaÃ§Ã£o HTTPS
    if B2_ENDPOINT and not B2_ENDPOINT.startswith("https://"):
        B2_ENDPOINT = f"https://{B2_ENDPOINT}"

    if B2_KEY_ID and B2_APP_KEY:
        s3_client = boto3.client(
            "s3",
            endpoint_url=B2_ENDPOINT,
            aws_access_key_id=B2_KEY_ID,
            aws_secret_access_key=B2_APP_KEY,
            config=Config(signature_version="s3v4")
        )
        B2_AVAILABLE = True
except Exception as e:
    logger.error(f"âŒ B2 Error: {e}")

# ==================== MODELOS GLOBAIS ====================
whisper_pipeline = None
qwen_model = None
qwen_tokenizer = None
yolo_model = None

# ==================== HELPERS ====================
def download_font():
    if not FONT_PATH.exists():
        try:
            url = "https://github.com/google/fonts/raw/main/ofl/oswald/Oswald-Bold.ttf"
            r = requests.get(url, timeout=30)
            with open(FONT_PATH, "wb") as f: f.write(r.content)
        except: pass
download_font()

def get_yolo():
    global yolo_model
    if yolo_model is None:
        try: yolo_model = YOLO("yolov8n.pt") 
        except: yolo_model = None
    return yolo_model

def clean_audio_deepfilter(input_path: Path) -> Path:
    try:
        output_dir = input_path.parent
        cmd = ["deepFilter", str(input_path), "-o", str(output_dir)]
        subprocess.run(cmd, check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        cleaned = output_dir / f"{input_path.stem}_DeepFilterNet3.wav"
        return cleaned if cleaned.exists() else input_path
    except:
        return input_path

def load_turbo_whisper():
    global whisper_pipeline
    if whisper_pipeline is None:
        try:
            logger.info("ðŸš€ Loading Whisper V3 (Flash Attention 2)...")
            whisper_pipeline = pipeline(
                "automatic-speech-recognition",
                model="openai/whisper-large-v3",
                torch_dtype=torch.float16,
                device="cuda:0",
                model_kwargs={"attn_implementation": "flash_attention_2"}
            )
        except:
            logger.warning("âš ï¸ Fallback Whisper (No FA2)")
            whisper_pipeline = pipeline(
                "automatic-speech-recognition",
                model="openai/whisper-large-v3",
                torch_dtype=torch.float16,
                device="cuda:0"
            )

def load_qwen():
    global qwen_model, qwen_tokenizer
    if qwen_model is None and AI_AVAILABLE:
        try:
            model_path = str(QWEN_MODEL_PATH)
            if not os.path.exists(model_path): model_path = "Qwen/Qwen2.5-7B-Instruct"
            
            logger.info("ðŸ§  Loading Qwen 2.5...")
            qwen_tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
            qwen_model = AutoModelForCausalLM.from_pretrained(
                model_path, device_map="auto", torch_dtype=torch.float16, trust_remote_code=True
            )
        except Exception as e:
            logger.error(f"âŒ Qwen Load Error: {e}")

# ==================== CORE FUNCTIONS ====================
def download_video(url: str) -> str:
    temp_file = TEMP_DIR / f"input_{uuid.uuid4().hex[:8]}.mp4"
    with requests.get(url, stream=True) as r:
        r.raise_for_status()
        with open(temp_file, 'wb') as f:
            for chunk in r.iter_content(chunk_size=8192): f.write(chunk)
    return str(temp_file)

class ActionDetector:
    def __init__(self, video_path): self.video_path = video_path
    def detect_high_energy_segments(self, threshold=70):
        if not CV2_AVAILABLE: return []
        cap = cv2.VideoCapture(self.video_path)
        fps = cap.get(cv2.CAP_PROP_FPS) or 30
        energy = []
        prev = None
        
        while True:
            ret, frame = cap.read()
            if not ret: break
            if int(cap.get(cv2.CAP_PROP_POS_FRAMES)) % int(fps) != 0: continue
            
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            gray = cv2.GaussianBlur(gray, (21, 21), 0)
            if prev is not None:
                delta = cv2.absdiff(prev, gray)
                thresh = cv2.threshold(delta, 25, 255, cv2.THRESH_BINARY)[1]
                score = (np.sum(thresh) / thresh.size) * 1000 # Boost scale
                energy.append({"time": cap.get(cv2.CAP_PROP_POS_MSEC)/1000, "score": min(score, 100)})
            prev = gray
        cap.release()
        return [e for e in energy if e["score"] > threshold]

def analyze_video_content(video_path, anime_name):
    load_turbo_whisper()
    load_qwen()
    if not whisper_pipeline or not qwen_model: raise Exception("AI Models Missing")
    
    # Extract & Clean Audio
    audio_path = TEMP_DIR / "temp.wav"
    subprocess.run(['ffmpeg', '-i', video_path, '-vn', '-ac', '1', '-ar', '16000', str(audio_path), '-y'], 
                   stderr=subprocess.DEVNULL)
    clean_audio = clean_audio_deepfilter(audio_path)
    
    # Transcribe
    result = whisper_pipeline(str(clean_audio), chunk_length_s=30, batch_size=24, return_timestamps=True, generate_kwargs={"language": "portuguese"})
    
    transcript = []
    for chunk in result.get("chunks", []):
        transcript.append(f"[{chunk['timestamp'][0]:.1f}s] {chunk['text']}")
        
    # Action
    detector = ActionDetector(video_path)
    actions = detector.detect_high_energy_segments()
    for a in actions: transcript.append(f"[{a['time']:.1f}s] [âš¡ ACTION SCENE]")
    
    full_text = "\n".join(sorted(transcript, key=lambda x: float(x.split(']')[0][1:-1])))
    
    prompt = f"""
    Analyze this anime '{anime_name}' script. Identify 3 VIRAL clips (40s-90s).
    Focus on Action [âš¡] and Dialogue.
    Script:
    {full_text[:20000]}
    
    Return JSON: [{{ "start": 10, "end": 60, "title": "EPIC FIGHT", "score": 99 }}]
    """
    
    inputs = qwen_tokenizer([prompt], return_tensors="pt").to(DEVICE)
    outputs = qwen_model.generate(**inputs, max_new_tokens=500, temperature=0.7)
    resp = qwen_tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    try:
        json_str = resp[resp.find("["):resp.rfind("]")+1]
        return json.loads(json_str)
    except: 
        logger.error(f"JSON Parse Error: {resp}")
        return []

# ==================== RENDERER (V2 COMPATIBLE) ====================
def apply_antishadowban(clip):
    # Mirror
    if random.choice([True, False]):
        if MOVIEPY_V1_COMPAT: clip = clip.fx(mirror_x)
        else: clip = clip.with_effects([MirrorX()])
    
    # Zoom Breathe (Manual impl compatible with both)
    def zoom(get_frame, t):
        img = get_frame(t)
        h, w = img.shape[:2]
        scale = 1 + 0.03 * (0.5 + 0.5 * math.sin(2 * math.pi * t / 5.0))
        nw, nh = int(w/scale), int(h/scale)
        x = (w-nw)//2
        y = (h-nh)//2
        return cv2.resize(img[y:y+nh, x:x+nw], (w,h))
    
    if CV2_AVAILABLE: clip = clip.fl(zoom)
    return clip

def processar_corte(video_path, cut, num, config):
    start, end = cut['start'], cut['end']
    
    # MoviePy V2 uses 'subclipped' usually, but 'subclip' might exist in dev releases.
    # We stick to V1 syntax if compat flag is on, else try V2
    with VideoFileClip(video_path) as video:
        if MOVIEPY_V1_COMPAT: clip = video.subclip(start, end)
        else: clip = video.subclipped(start, end)
        
        if config.get("antiShadowban"): clip = apply_antishadowban(clip)
        
        # Smart Crop
        zoom = 1.15
        w, h = clip.w, clip.h
        nw, nh = w/zoom, h/zoom
        
        # YOLO Center
        cx, cy = w/2, h/2
        try:
            yolo = get_yolo()
            if yolo:
                res = yolo(clip.get_frame(clip.duration/2), verbose=False)
                # Logic to find person center... skipped for brevity, using center default if fail
        except: pass
        
        x1 = max(0, min(cx - nw/2, w - nw))
        y1 = max(0, min(cy - nh/2, h - nh))
        
        clip = clip.crop(x1=x1, y1=y1, width=nw, height=nh)
        
        # Resize to 1080p width
        target_w, target_h = 1080, 1920
        scale = target_w / clip.w
        clip = clip.resize(scale)
        
        # Composite
        clip = clip.set_position("center")
        bg_color = ColorClip(size=(target_w, target_h), color=(20,20,20), duration=clip.duration)
        
        final = CompositeVideoClip([bg_color, clip], size=(target_w, target_h))
        
        out = OUTPUT_DIR / f"cut_{num}_{uuid.uuid4().hex[:6]}.mp4"
        
        # NVENC
        params = ['-pix_fmt', 'yuv420p']
        codec = 'libx264'
        preset = 'ultrafast'
        
        if torch.cuda.is_available():
            codec = 'h264_nvenc'
            preset = 'p4'
            params.extend(['-rc:v', 'constqp', '-cq:v', '20', '-b:v', '0'])
        
        final.write_videofile(str(out), codec=codec, preset=preset, ffmpeg_params=params, 
                             verbose=False, logger=None, threads=4)
        return str(out)

def upload_to_b2(path):
    if not B2_AVAILABLE: return None
    key = f"animecut/v9/{os.path.basename(path)}"
    s3_client.upload_file(path, B2_BUCKET, key)
    return s3_client.generate_presigned_url('get_object', Params={'Bucket': B2_BUCKET, 'Key': key}, ExpiresIn=86400)

def handler(event):
    gc.collect()
    inp = event.get("input", {})
    
    url = inp.get("video_url")
    if not url: return {"error": "No URL"}
    
    path = download_video(url)
    config = {"animeName": inp.get("animeName", "Anime"), "antiShadowban": True}
    
    cuts = []
    if AI_AVAILABLE:
        try: cuts = analyze_video_content(path, config['animeName'])
        except: pass
        
    if not cuts: # Manual Fallback
        cuts = [{"start": 0, "end": 60, "title": "Manual Cut"}]
    
    res = []
    for i, cut in enumerate(cuts):
        out = processar_corte(path, cut, i, config)
        url = upload_to_b2(out)
        res.append({"url": url, "title": cut.get("title")})
        
    return {"status": "success", "cuts": res}

if __name__ == "__main__":
    runpod.serverless.start({"handler": handler})
